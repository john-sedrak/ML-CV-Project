{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-sedrak/ML-CV-Project/blob/main/DS_ML_Project_Day_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtftuTyyi-HY"
      },
      "source": [
        "\n",
        "#<center> DS ML Capstone Project: Celebrity Face Recognition\n",
        "\n",
        "\n",
        "##### <center>Original work: [Sports Celebrity Image Classification — codebasics](https://youtube.com/playlist?list=PLeo1K3hjS3uvaRHZLl-jLovIjBP14QTXc)\n",
        "##### <center> Prepared by: Ahmed Mokhtar\n",
        "\n",
        "---\n",
        "\n",
        "## Preface\n",
        "Greetings, data scientists! The semester plan has concluded. Throughout the semester, you have all shown a great deal of diligence and dedication, and we could not be more proud of you! The time has finally come for you to test and expand your knowledge. At this point, it is safe to say that you are all familiar with [structured](https://wordlift.io/blog/en/entity/structured-data/) data. To change things up, we wanted to expose you to [unstructured](https://searchbusinessanalytics.techtarget.com/definition/unstructured-data) data in a fun and exciting project that serves as an introduction to the world of [computer vision](https://www.ibm.com/topics/computer-vision). It is an end-to-end data science project which is full of new and exciting concepts for you to learn. It is slightly challenging, because we are confident in your abilities! \n",
        "\n",
        "<img src=\"https://i.imgur.com/i1bSfkM.png\" alt=\"celebration\" width=\"100%\">\n",
        "\n",
        "## Part I: Introduction\n",
        "\n",
        "Watch this video before you proceed: [Data Science & Machine Learning Project - Part 1 Introduction | Image Classification](https://www.youtube.com/watch?v=qWXXHjV3JHI&list=PLeo1K3hjS3uvaRHZLl-jLovIjBP14QTXc&index=1)<br><br>\n",
        "\n",
        "In this project, you will build an image face recognition program from start to finish. First, you will have to **collect** your data. Then you will clean your data, and **engineer features** out of it. Once the data is ready, you will use it to **train** a machine leaning algorithm to classify faces. You will have to experiment with and **evaluate** different models in order to find the optimal one for this problem. Finally, you will **deploy** your model on the cloud.  \n",
        "\n",
        "By the end of this project, you should be able to:\n",
        "\n",
        "*   Scrape the web for data (optional).\n",
        "*   Know what [OpenCV](https://opencv.org) is and what it does.\n",
        "*   Know how images are represented in python.\n",
        "*   Know what Haar cascade classifiers are and how to use them in python.\n",
        "*   Navigate and interact with directories using code.\n",
        "*   Know what wavelet decomposition is and how to apply it in python.\n",
        "*   Train a classical ML classifier on image data.\n",
        "*   Build a python [Flask](https://flask.palletsprojects.com/en/2.0.x/) server for your model.\n",
        "*   Build an [API](https://en.wikipedia.org/wiki/API) for your model.\n",
        "*   Build an HTML document for your application (optional).\n",
        "*   Connect your API to the frontend using JavaScript (optional).\n",
        "*   Deploy your model to production using [AWS](https://aws.amazon.com).\n",
        "\n",
        "Without keeping you waiting any further, let's dive into the project!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLOKHXBrXHwB"
      },
      "source": [
        "## Part II: Data Collection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBH7E8Bo7a7T"
      },
      "source": [
        "Watch this video before you proceed: [Data Science & Machine Learning Project - Part 2 Data Collection | Image Classification](https://www.youtube.com/watch?v=m1dQ38qDABw&list=PLeo1K3hjS3uvaRHZLl-jLovIjBP14QTXc&index=2&t=8s)<br><br>\n",
        "\n",
        "We want to create dataset containing 50 images for each class (person). Fortunately, the web is home to thousands of pictures that could be easily downloaded. As stated in the video, there are 4 ways you can get the images you need.\n",
        "\n",
        "1.   **Downlaoding them manually:** This is tedious and not recommended.\n",
        "2.   **Scraping the web:** This involves using software like [Selenium](https://www.guru99.com/introduction-to-selenium.html), or [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) in order to extract information from HTML elements. This requires you to write a moderate amount of code at the start, but it is much easier than manual extraction. You also have to make sure that you are legally allowed to possess the data you are scraping (i.e. no personal or copyrighted data).\n",
        "3.   **Using a pre-made software for your scraping task:** for example, the [fatkun](https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf?hl=en) chrome extension is made for the purpose of scraping images. This saves you time, but is not as flexible as Selenium or Beautiful Soup (recommended).\n",
        "4.   **Buying the data:** You can buy the needed images from the parties that own them.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PB6sPQ8LYRx"
      },
      "source": [
        "<mark>Task: Pick 5 celebrities, then use the [fatkun](https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf?hl=en) chrome extension to download 50 photos for each of them. \n",
        "\n",
        "<mark>Alternative task 1: Use Selenium to scrape the images. The code is prepared for you, only need to understand the code and what it does.\n",
        "\n",
        "<mark>Alternative task 2: You can use 50 photos of yourself or people you know to make your dataset.\n",
        "\n",
        "The folowing code uses Selenium to scrape images from the web.\n",
        "\n",
        "Credit for the code goes to: [Image Scraping With Python](https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d)\n",
        "\n",
        "Make sure you read the article carefully and understand what the code does. I have also added extra comments to the code to explain it as much as i can.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYmLuFI0-KXk"
      },
      "source": [
        "import time\n",
        "import requests \n",
        "import io\n",
        "import hashlib\n",
        "import os\n",
        "import sys \n",
        "from os import path\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from PIL import Image, ImageDraw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y-xAbJ-cK-t"
      },
      "source": [
        "# install the selenium framework\n",
        "!pip install selenium \n",
        "\n",
        "# to update ubuntu to correctly run apt install\n",
        "!apt-get update \n",
        "\n",
        "# chromedriver is a tool for autimated web navigation\n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "# linux command to copy files from src directory to dest directory\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin \n",
        "\n",
        "# telling python to look in this path for chromedriver\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "\n",
        "# this makes sure we scrape the web without windows popping up or any form of user interface\n",
        "chrome_options.add_argument('--headless')\n",
        "\n",
        "# disable sandboxing (https://www.google.com/googlebooks/chrome/med_26.html)\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "\n",
        "# disable shared memory space to avoid crashes\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oCgwW5PUG5j"
      },
      "source": [
        "The functions below facilitate image scraping from google:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcAxqupjbV7t"
      },
      "source": [
        "# once we get the URL of an image, we use the fetch_image_urls_util function\n",
        "def fetch_image_urls_util(url,driver_path):\n",
        "    images = []\n",
        "    # Open main window with the URL\n",
        "    with webdriver.Chrome(executable_path=driver_path, options=chrome_options) as wd:\n",
        "\n",
        "        # Switch to the new window and open URL B\n",
        "        try:\n",
        "            wd.get(url)\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "        # find images with the class 'n3VNCb'\n",
        "        thumbnail_results = wd.find_elements_by_css_selector(\"img[class ='n3VNCb']\")\n",
        "\n",
        "        # get the 'src' or the raw image\n",
        "        for img in thumbnail_results:\n",
        "            if img.get_attribute('src') and 'http' in img.get_attribute('src'):\n",
        "                images.append(img.get_attribute('src'))\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def fetch_image_urls(query:str, max_links_to_fetch:int, wd, sleep_between_interactions:int=1,driver_path= None, target_path = None, search_term = None):\n",
        "    \n",
        "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
        "\n",
        "    # scroll to the end of the page to load it fully\n",
        "    def scroll_to_end(wd):\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(sleep_between_interactions)    \n",
        "    \n",
        "    # build the google query\n",
        "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
        "\n",
        "    # load the page\n",
        "    wd.get(search_url.format(q=query))\n",
        "\n",
        "    image_urls = set()\n",
        "    image_count = 0\n",
        "    image_count2 = 0\n",
        "    total_saved = 0\n",
        "    results_start = 0\n",
        "    i = 0\n",
        "    d = {}\n",
        "\n",
        "    while image_count < max_links_to_fetch:\n",
        "        scroll_to_end(wd)\n",
        "\n",
        "        # get all image thumbnail results\n",
        "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
        "        number_results = len(thumbnail_results)\n",
        "        print(f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\")\n",
        "        \n",
        "        # for all the clickable thumbnails\n",
        "        for img in thumbnail_results[50:number_results]:\n",
        "            # try to click every thumbnail such that we can get the real image behind it\n",
        "            try:\n",
        "                img.click()\n",
        "                time.sleep(sleep_between_interactions)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                continue\n",
        "            \n",
        "            # get the clickable element in the thumbnail\n",
        "            links = wd.find_elements_by_css_selector(\"a[jsname='sTFXNd']\")\n",
        "\n",
        "            # for each clickable element\n",
        "            for link in links:\n",
        "                # get the target URL\n",
        "                if link.get_attribute('href') and 'http' in link.get_attribute('href'):\n",
        "                    if link.get_attribute('href') not in d:\n",
        "                        d[link.get_attribute('href')] = True\n",
        "                        # get the raw image from the URL\n",
        "                        getactualurl = fetch_image_urls_util(link.get_attribute('href'), driver_path)\n",
        "                    for imageurl in getactualurl:\n",
        "                        if imageurl is not None:\n",
        "                            # append the image URL to the list\n",
        "                            image_urls.add(imageurl)\n",
        "            # the count of extracted URLs\n",
        "            image_count2 = len(image_urls)\n",
        "            # save every 10th of the max URLs\n",
        "            if image_count2 >= max_links_to_fetch/10:\n",
        "                print(f\"Found: {len(image_urls)} image links, saving!\")\n",
        "                try:    \n",
        "                    for elem in image_urls:\n",
        "\n",
        "                        if persist_image(target_folder,elem):\n",
        "                            total_saved += 1\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "                # reset the set of URLs\n",
        "                image_urls = set()\n",
        "                d = {}\n",
        "            # total URL count\n",
        "            image_count += image_count2\n",
        "            print(\"Total: \", total_saved, \" images!\")\n",
        "            if total_saved >= max_links_to_fetch:\n",
        "                break\n",
        "\n",
        "        # if we reach out target number we stop \n",
        "        if total_saved >= max_links_to_fetch:\n",
        "            print(f\"Found: {total_saved} image links, done!\")\n",
        "            break\n",
        "        else:\n",
        "            # else we press the load more button and look for more images\n",
        "            print(\"Found:\", image_count, \"image links, looking for more ...\")\n",
        "            time.sleep(30)\n",
        "            return\n",
        "            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
        "            if load_more_button:\n",
        "                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
        "\n",
        "        # move the result startpoint further down\n",
        "        results_start = image_count\n",
        "\n",
        "    print(len(image_urls))\n",
        "    return image_urls\n",
        "\n",
        "\n",
        "# takes the image URL and saves it on disk\n",
        "def persist_image(folder_path:str,url:str):\n",
        "    try:\n",
        "        image_content = requests.get(url).content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not download {url} - {e}\")\n",
        "\n",
        "    try:\n",
        "        image_file = io.BytesIO(image_content)\n",
        "        image = Image.open(image_file).convert('RGB')\n",
        "        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
        "\n",
        "        if path.exists(file_path):\n",
        "            print(f\"DUPLICATE - image {url} already exists\")\n",
        "            return False\n",
        "\n",
        "        with open(file_path, 'wb') as f:\n",
        "            image.save(f, \"JPEG\", quality=85)\n",
        "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not save {url} - {e}\")\n",
        "        \n",
        "  \n",
        "# takes the search query and the number of images and fetches them    \n",
        "def search_and_download(search_term:str,driver_path:str,target_path='./datasets',number_images=50):\n",
        "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
        "\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "\n",
        "    with webdriver.Chrome(executable_path=driver_path, options=chrome_options) as wd:\n",
        "        res = fetch_image_urls(search_term, number_images, wd=wd, sleep_between_interactions=0.5,driver_path= driver_path,target_path= target_path,search_term=search_term)\n",
        "    try:    \n",
        "        for elem in res:\n",
        "            persist_image(target_folder,elem)\n",
        "    except Exception as e:\n",
        "        print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJD6nMHnUMf7"
      },
      "source": [
        "Finally, we call the `search_and_download` function to get our images. Feel free to change the names in the query to any 5 characters' names you want (they must be popular, have a face and 2 eyes)\n",
        "\n",
        "<font color=\"orange\">N.B. This is a really slow process and it might take a serious amount of time</font><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YxRYl9xdnzV"
      },
      "source": [
        "query = [\"Serena Williams\", \"Lionel Messi\", \"Maria Sharapova\", \"Roger Federer\", \"Virat Kohli\"]\n",
        "\n",
        "for q in query:\n",
        "    search_and_download(q, 'chromedriver', number_images=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcOukcvYTuxX"
      },
      "source": [
        "Your dataset directory should separate classes in different folders. The directory should look like this in the end (with your class names instead):<br>\n",
        "![Data](https://i.imgur.com/4qNtole.png)<br><br>\n",
        "\n",
        "Since we will be using this dataset for the rest of the project, it is wise to upload it to your google drive so you wouldn't have to upload it every time you access your notebooks. Download the dataset if it is not available locally already. Colab does not allow us to download folders, so you will have to zip it first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNaUYhHHYpXh"
      },
      "source": [
        "# zip -r <zipped file destination> <folder to be zipped>\n",
        "!zip -r /content/dataset.zip /content/dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVj7Kn5nfBEc"
      },
      "source": [
        "Now download the zipped dataset by right clicling on it.\n",
        "\n",
        "![Download](https://i.imgur.com/Nfj161Y.png)<br><br>\n",
        "\n",
        "Finally, unzip the file and upload the folder to your google drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMTQn-oPIaFl"
      },
      "source": [
        "Congratulations! You have successfully created your own image dataset. The most confusing part of this project is over! In the next part, we will clean our dataset and extract faces from the images. In the subsequent notebooks, You will have the option to use a pre-made dataset. Therefore, you will be able to progress even if you did not finish this notebook.<br><br>\n",
        "\n",
        "Enjoy the rest of your day! ❤️<br><br>\n",
        "\n",
        "<center><img src=\"https://i.pinimg.com/originals/55/f5/fd/55f5fdc9455989f8caf7fca7f93bd96a.gif\" width=\"30%\">"
      ]
    }
  ]
}